#parameters:

encoder_layers = 5
decoder_layers = 1

encoder_hidden_size = 300
decoder_ner_hidden_size = 300
attention_hidden_size = 300
num_filters = 100

encoder_lr = 0.0005
decoder_lr = 0.0005
decoder_ner_lr = 0.0005

embedding_dim_chars = 150
embedding_dim_words = 300

attention_type = 'hybrid'

batch_size = 10
num_epochs = 50
MAX_LENGTH = 300
skip_training = False

------------------------------------------

# bigger model

encoder_hidden_size = 450
decoder_ner_hidden_size = 450
attention_hidden_size = 450
num_filters = 150


embedding_dim_chars = 150
embedding_dim_words = 300

_________________________________________

# swedish small model

encoder_layers = 3
decoder_layers = 1

encoder_hidden_size = 150
decoder_ner_hidden_size = 150
attention_hidden_size = 150
num_filters = 50

encoder_lr = 0.0005
decoder_lr = 0.0005
decoder_ner_lr = 0.0005

embedding_dim_chars = 100
embedding_dim_words = 300

attention_type = 'hybrid'

batch_size = 25
num_epochs = 50
MAX_LENGTH = 300
skip_training = False




-----------------------------------------------------

- LibriSpeech, with batch 10
57364991 - LibriSpeech with bigger model
 - Parliament, trained with batch of 5
- Swedish, with batch of 50
57369900 - Swedish small


